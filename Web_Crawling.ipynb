{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['21076849694', '13810664630', '20850829749', '21285691276', '21393999339', '13611653308', '17586234530']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import requests, pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "from datetime import datetime\n",
    "\n",
    "# JS로 실행되는 Paging을 Crawling하기 위해서는 selenium Library를 이용하여 직접 화면의 JS를 실행해야한다.\n",
    "# ex) \n",
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from itertools import count\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# \n",
    "# def crawling_goobne():\n",
    "#     url = 'http://www.goobne.co.kr/store/search_store.jsp'\n",
    "# \n",
    "#     # \\U는 유니코드로 인식되기 때문에 \\\\U와 같이 escape 처리했다.\n",
    "#     wd = webdriver.Chrome('C:\\\\Users\\samsung\\python\\webdriver\\chromedriver.exe')\n",
    "#     wd.get(url)\n",
    "# \n",
    "#     RESULT_DIRECTORY = '__results__/crawling'\n",
    "#     results = []\n",
    "#     for page in count(1):\n",
    "#         script = 'store.getList(%d)' % page  # 굽네치킨에서 사용하는 페이지를 이동시키는 js 코드\n",
    "#          wd.execute_script(script)  # js 실행\n",
    "#          time.sleep(5)              # 크롤링 로직을 수행하기 위해 5초정도 쉬어준다.\n",
    "# \n",
    "#         html = wd.page_source\n",
    "#         bs = BeautifulSoup(html, 'html.parser')\n",
    "#         tag_body = bs.find('tbody', attrs={'id': 'store_list'})\n",
    "#         tags_tr = tag_body.findAll('tr')\n",
    "#         # print(tags_tr)\n",
    "# \n",
    "#         if tags_tr[0].get('class') is None:  # 맨 마지막 페이지인 102에서는 class='on lows'가 없다. => 종료 조건\n",
    "#             break\n",
    "# \n",
    "#         for tag_tr in tags_tr:\n",
    "#             strings = list(tag_tr.strings)\n",
    "#             name = strings[1]\n",
    "#             address = strings[6]\n",
    "#             results.append((name, address))\n",
    "# \n",
    "#     table = pd.DataFrame(results, columns=['name', 'address'])\n",
    "#     table.to_csv('{0}/table_goobne.csv'.format(RESULT_DIRECTORY), encoding='utf-8', mode='w')\n",
    "\n",
    "def get_html(_url, _id):\n",
    "    r = requests.get(_url + _id)\n",
    "    html = BeautifulSoup(r.content, 'html.parser')\n",
    "    \"\"\"\n",
    "    Function select is a copy selector reading from tag. \n",
    "        div { id: section_review, class: detail_cont detail_customer_review }\n",
    "            div { id: _review_filter_container }\n",
    "                ul { id: _review_list, class: lst_review }\n",
    "                    li { class: thumb_nail } or li { class: thumb_nail_open } javascript function execution !\n",
    "    BeatifulSoup API\n",
    "        select(), contents(), extract()\n",
    "    \"\"\"\n",
    "    li_size = len(html.find_all(\"li\", attrs={'class': 'thumb_nail'}))\n",
    "    crawling_arr_set = []\n",
    "    while li_size:\n",
    "        average_star = html.select(\"#_review_list > li:nth-child(%d) > div > div.avg_area > a > span.curr_avg > strong\" % li_size)[0].text\n",
    "        store = html.select(\"#_review_list > li:nth-child(%d) > div > div.avg_area > span > span:nth-child(1)\" % li_size)[0].text\n",
    "        user_name = html.select(\"#_review_list > li:nth-child(%d) > div > div.avg_area > span > span:nth-child(2)\" % li_size)[0].text\n",
    "        write_date = html.select(\"#_review_list > li:nth-child(%d) > div > div.avg_area > span > span:nth-child(3)\" % li_size)[0].text\n",
    "        title_text = html.select(\"#_review_list > li:nth-child(%d) > div > p > strong\" % li_size)[0].text\n",
    "        body_text = html.select(\"#_review_list > li:nth-child(%d) > div > div.atc\" % li_size)[0].text\n",
    "        # print(li_size)\n",
    "        # print(average_star, store, user_name, write_date, title_text, body_text)\n",
    "        crawling_arr_set.append({\n",
    "            \"aver_star\": average_star,\n",
    "            \"user\": user_name,\n",
    "            \"store\": store,\n",
    "            \"date\": write_date,\n",
    "            \"title\": title_text,\n",
    "            \"body\": body_text,\n",
    "            \"product_id\": _id\n",
    "        })\n",
    "        li_size -= 1\n",
    "    return crawling_arr_set\n",
    "\n",
    "'''\n",
    "    Chrome Naver Shopping Raw URL\n",
    "    https://search.shopping.naver.com/detail/detail.nhn?nvMid=21076849694&NaPm=ct%3Dkad8qf08%7Cci%3D0yW0002VbGbs1XUxwvkS%7Ctr%3Dpla%7Chk%3D6066b0a1499f4f77890d54202d9808691c54a75d\"\n",
    "    https://search.shopping.naver.com/detail/detail.nhn?nvMid=13810664630&NaPm=ct%3Dkbkecz08%7Cci%3Da630b02bff9323064460419c72916238067e54e6%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D6a4ad9a61c99a492bd2beb295b5e65864a233a92\"\n",
    "    parse.unquote() is Decoding the URL parameter.\n",
    "'''\n",
    "\n",
    "NAVER_SHOPPING_BASE_URL = \"https://search.shopping.naver.com/detail/detail.nhn?nvMid=\"\n",
    "KUKU_AC_25W20FPMO = \"21076849694\"\n",
    "DYSON_PURE_COOL_TP_04 = \"13810664630\"\n",
    "ITC_PURIWAY_PW_150 = \"20850829749\"\n",
    "LG_PURICARE_AS300DWFA = \"21285691276\"\n",
    "WINIX_FRIME_APRM833_JWK = \"21393999339\"\n",
    "SAMSUNG_BLUESKY_AX34N3020WWD = \"13611653308\"\n",
    "SKMAGIC_ACL_120Z0 = \"17586234530\"\n",
    "ELECTROMAN_DAP_2215NAWHEM = \"22163383505\"\n",
    "\n",
    "product_arr = [KUKU_AC_25W20FPMO,DYSON_PURE_COOL_TP_04,ITC_PURIWAY_PW_150,LG_PURICARE_AS300DWFA,\n",
    "               WINIX_FRIME_APRM833_JWK, SAMSUNG_BLUESKY_AX34N3020WWD, SKMAGIC_ACL_120Z0]\n",
    "print(product_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5b5aa6e5a379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNAVER_SHOPPING_BASE_URL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKUKU_AC_25W20FPMO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Raw : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1 : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2 : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\t\\s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ],
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error"
    }
   ],
   "source": [
    "res = get_html(NAVER_SHOPPING_BASE_URL, KUKU_AC_25W20FPMO)[2]['body']\n",
    "print(\"Raw : \", res)\n",
    "print(\"1 : \", res.splitlines())\n",
    "print(\"2 : \", res.strip('\\n\\t\\s'))\n",
    "import re\n",
    "print(\"3 : \", re.sub(r\"[\\n\\t\\s]*\", \"\", res))\n",
    "print(\"4 : \", res.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "res = []\n",
    "for product_id in product_arr:\n",
    "    res.append(get_html(NAVER_SHOPPING_BASE_URL, product_id))\n",
    "    print(len(res))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-97def67c1ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Jupiter에서 driver 정상 동작 안함.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# TypeError: to_capabilities() missing 1 required positional argument: 'self' 발생!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\CheonYoungJo\\\\Desktop\\\\Local_Repo\\\\Algorithm\\\\chromedriver.exe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdesired_capabilities\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mdesired_capabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mdesired_capabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: to_capabilities() missing 1 required positional argument: 'self'"
     ],
     "ename": "TypeError",
     "evalue": "to_capabilities() missing 1 required positional argument: 'self'",
     "output_type": "error"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options\n",
    "options.headless = True\n",
    "# Jupiter에서 driver 정상 동작 안함. \n",
    "# TypeError: to_capabilities() missing 1 required positional argument: 'self' 발생!\n",
    "driver = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\CheonYoungJo\\\\Desktop\\\\Local_Repo\\\\Algorithm\\\\chromedriver.exe\", options=options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "#   if existReview:     리뷰가 존재하면 실행\n",
    "#       page = parsing  페이지 몇개인지 찾기\n",
    "#       while page:     모든 페이지 실행\n",
    "#          get_tag\n",
    "#          if existPage:   페이지가 더 있으면 스크립트 실행\n",
    "#             driver.execution_script()\n",
    "#             sleep()\n",
    "\n",
    "def get_html2(_url, _id):\n",
    "    r = requests.get(_url + _id)\n",
    "    html = BeautifulSoup(r.content, 'html.parser')\n",
    "    li_size = len(html.find_all(\"li\", attrs={'class': 'thumb_nail'}))\n",
    "    print(li_size)\n",
    "    \n",
    "get_html2(\"https://search.shopping.naver.com/detail/detail.nhn?nvMid=\", \"2131231231223213\")\n",
    "\n",
    "#container > div\n",
    "#_review_paging > a.next_end\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'python_test3'}\n",
      "{'name': 'els-1', 'cluster_name': 'elk-bigdata', 'cluster_uuid': 'J9cMv8YXRkeq0ThyNLAYYQ', 'version': {'number': '7.6.1', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': 'aa751e09be0a5072e8570670309b1f12348f023b', 'build_date': '2020-02-29T00:15:25.529771Z', 'build_snapshot': False, 'lucene_version': '8.4.0', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n",
      "(20, [])\n",
      "(20, [])\n",
      "(20, [])\n",
      "(20, [])\n",
      "(20, [])\n",
      "(20, [])\n",
      "(20, [])\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "{'_shards': {'total': 2, 'successful': 2, 'failed': 0}}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 78
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Elasticsearch 적재\n",
    "    Index Name Example : external-customer_review-제조사-모델명-200618\n",
    "\"\"\"\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import datetime, pprint, random, time\n",
    "import pandas as pd\n",
    "\n",
    "def make_index(_es, name):\n",
    "    if es.indices.exists(index=name):\n",
    "        es.indices.delete(index=name)\n",
    "    print(es.indices.create(index=name))\n",
    "\n",
    "\n",
    "es = Elasticsearch([\"13.125.246.197:9200\", \"52.79.215.253:9200\", \"52.79.250.228:9200\"])\n",
    "index_name = \"python_test3\"\n",
    "make_index(es, index_name)\n",
    "print(es.info())\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\"\"\"\n",
    "    중복 저장 X \n",
    "    DateFrame use to changing the datetime type to pandas.timestamp type.\n",
    "    Elasticsearch의 Bulk 함수를 이용하여 많은 데이터를 적재시키는 코드로 body array를 만들어 저장하는 방법.\n",
    "\"\"\"\n",
    "\n",
    "for office in res:\n",
    "    body = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": x\n",
    "        }\n",
    "        for x in office\n",
    "    ]\n",
    "    res = helpers.bulk(es, body)\n",
    "    pp.pprint(res)\n",
    "    time.sleep(1)\n",
    "    \n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "# Test Code\n",
    "# _id = ['test1', 'test2', 'test3', 'test4', 'test5', 'test6', 'test7']\n",
    "# _int = [1,2,3,4,5,6,7]\n",
    "# _bool = [True, False, True, False, True, False, True]\n",
    "# _date = [datetime.datetime.now(), datetime.datetime.now(), datetime.datetime.utcnow(), \n",
    "#          datetime.datetime.utcnow(), datetime.datetime.now(), datetime.datetime.utcnow(), datetime.datetime.now()]\n",
    "# \n",
    "# df = pd.DataFrame([x for x in zip(_id, _int, _bool, _date)], columns=['ID', 'INT', 'BOOL', 'Dat'])\n",
    "# print(df)\n",
    "# for i in zip(df['ID'], df['INT'], df['BOOL'], df['Dat']):\n",
    "#     print(i, type(i), i[3], type(i[3]))\n",
    "#     a = i[3].to_pydatetime()\n",
    "#     print(a, type(a))\n",
    "# \n",
    "# body = [\n",
    "#     {\n",
    "#         \"_index\": \"python_test2\",\n",
    "#         \"_source\": {\n",
    "#             \"Id\": x[0],\n",
    "#             \"In\": random.randint(0,10),\n",
    "#             \"Bol\": x[2],\n",
    "#             \"Dat\": datetime.datetime.utcnow()\n",
    "#         }\n",
    "#     }\n",
    "#     # for x in zip(_id, _int, _bool, _date)\n",
    "#     for x in zip(df['ID'], df['INT'], df['BOOL'], df['Dat'])\n",
    "# ]\n",
    "# \n",
    "# pp.pprint(body)\n",
    "# \n",
    "# # res = es.bulk(index=\"python_test\", body=body, doc_type='doc')\n",
    "# res = helpers.bulk(es, body)\n",
    "# pp.pprint(res)\n",
    "# \n",
    "# es.indices.refresh(index=\"python_test2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}